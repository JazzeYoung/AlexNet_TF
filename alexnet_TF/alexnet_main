#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue May  2 10:23:31 2017

@author: ai-yang
"""
# AlexNet with tensorflow

import numpy
import tensorflow as tf
import time

# initialization
# MNIST

INITIAL_LEARNING_RATE = 0.01
LEARNING_RATE_DECAY_FACTOR = 0.1
MOMENTUM = 0.9
WEIGHT_DACAY = 0.005
MOVING_AVERAGE_DECAY = 0.005
# modeling--inference
def model(images, train=False):
    # Model Stucture
    parameters = []
#    weights = tf.Variable()  use this form in trainning
#    biases = tf.Variable()
    # Layer 1:conv1
    with tf.variable_scope('conv1') as scope:
        kernel = tf.Variable(tf.truncated_normal([11,11,3,96], dtype=tf.float32, stddev=0.01), name = 'weights')
        conv1 = tf.nn.conv2d(images, kernel, strides=[1, 4, 4, 1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[96], dtype=tf.float32), trainable=True, name='biases')
        conv1 = tf.nn.relu(tf.nn.bias_add(conv1, biases), name=scope)
        parameters += [kernel, biases]
        
    norm1 = tf.nn.lrn(conv1, depth_radius=3, bias=2, alpha=1e-4, beta=0.75, name='norm1')
    pool1 = tf.nn.max_pool(norm1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID', name='pool1')
    # Layer 2:conv2
    with tf.variable_scope('conv2') as scope:
        kernel = tf.Variable(tf.truncated_normal([5, 5, 48, 256], dtype=tf.float32, stddev=0.01), name='weights')
        conv2 = tf.nn.conv2d(pool1, kernel, strides=[1,1,1,1], padding='SAME')
        biases = tf.Variable(tf.constant(1.0, shape=[256], dtype=tf.float32), trainable=True, name='biases')
        conv2 = tf.nn.relu(tf.nn.bias_add(conv2, biases), name=scope)
        parameters += [kernel, biases]
    norm2 = tf.nn.lrn(conv2,depth_radius=3, bias=2, alpha=1e-4, beta=0.75, name='norm2')
    pool2 = tf.nn.max_pool(norm2, ksize=[1,3, 3, 1], strides=[1,2,2,1], padding='VALID', name='pool2')
    # Layer 3:conv3
    with tf.variable_scope('conv3') as scope:
        kernel = tf.Variable(tf.truncated_normal([3,3,256,384], dtype=tf.float32, stddev=0.01), name='weights')
        conv3 = tf.nn.conv2d(pool2, kernel, strides=[1,1,1,1], padding='SAME')
        biases = tf.Variable(tf.constant(0.0, shape=[384], dtype=tf.float32), trainable=True, name='biases')
        conv3 = tf.nn.relu(tf.nn.bias_add(conv3, biases), name=scope)
        parameters += [kernel, biases]
    #pool3 =
    
    with tf.variable_scope('conv4') as scope:
        kernel = tf.Variable(tf.truncated_normal([3,3,192, 256], dtype=tf.float32, stddev=0.01), name='weights')
        conv4 = tf.nn.conv2d(conv3, kernel, strides=[1,1,1,1], padding='SAME')
        biases = tf.Variable(tf.constant(1.0, shape=[256], dtype=tf.float32), trainable=True, name='biases')
        conv4 = tf.nn.relu(tf.nn.bias_add(conv4, biases), name=scope)
        parameters += [kernel, biases]
        
    with tf.variable_scope('conv5') as scope:
        kernel = tf.Variable(tf.truncated_normal([3,3, 192, 384], dtype=tf.float32, stddev=0.01), name='weights)
        conv5 = tf.nn.conv2d(conv4, kernel, strides=[1,1,1,1], padding='SAME')
        biases = tf.Variable(tf.constant(1.0, shape=[385], dtype=tf.float32), trainable=True, name='biases')
        conv5 = tf.nn.relu(tf.nn.bias_add(conv5, biases))
        parameters += [kernel, biases]
    pool5 = tf.nn.max_pool(conv5, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID', name='pool5')
    
    with tf.variable_scope('fc1') as scope:
        weights = tf.Variable(tf.truncated_normal([43264,4096], dtype=tf.float32, stddev=0.01), name='weights')
        biases = tf.Variable(tf.constant(0.0, shape=[4096], dtype=tf.float32), trainable=True, name='biases')
        if train:
            drop1 = tf.nn.dropout(pool5, keep_prob=0.5, name='drop1')
            fc1 = tf.nn.relu(tf.matmul(drop1, weights)+biases, name=scope)
        fc1 = tf.nn.relu(tf.matmul(pool5, weights)+biases, name=scope)
        parameters += [weights, biases]
        
    with tf.variable_scope('fc2') as scope:
        weights = tf.Variable(tf.truncated_normal([4096, 4096], dtype=tf.float32, stddev=0.01), name='weights')
        biases = tf.Variable(tf.constant(0.0, shape=[4096], dtype=tf.float32), trainable=True, name='biases')
        if train:
            drop2 = tf.nn.dropout(fc1, keep_prob=0.5, name='drop2')
            fc2 = tf.nn.relu(tf.matmul(drop2, weights)+biases, name=scope)
        fc2 = tf.nn.relu(tf.matmul(fc1, weights)+biases, name=scope)'
        parameters += [weights, biases]
    
    with tf.variable_scope('fc3') as scope:
        weights = tf.Variable(tf.truncated_normal([4096, 1000], dtype=tf.float32, stddev=0.01), name='weights')
        biases = tf.Variable(tf.constant(0.0, shape=[1000], dtype=tf.float32), trainable=True, name='biases')
        fc3 = tf.nn.relu(tf.matmul(fc2, weights)+biases, name=scope)
        parameters += [weights, biases]
        
    with tf.variable_scope('softmax_linear') as scope:
        weights = tf.Variable(tf.truncated_normal([1000, 10], dtype=tf.float32, stddev=0.01), name='weights')
        biases = tf.Variable(tf.constant(0.0, shape=[10],dtype=tf.float32), name='biases')
        softmax_linear = tf.add(tf.matmul(fc3, weights), biases, name=scope)
        parameters += [weights, biases]
        
    return softmax_linear, parameters


def loss(output, parameters, label, train=False):
    # Cross entropy + L2 weight regularizer
    #Cross entropy loss
    labels = tf.cast(labels, tf.int64)
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=label, name='xentropy')
    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')
    # L2 regularization for the fully connected parameters.
    if train:
        regularizers = (tf.nn.l2_loss(parameters))
        # Add the regularization term to the loss.
        loss += 5e-4 * regularizers
    tf.add_to_collection('losses', loss)
    
    return tf.add_n(tf.get_collection('losses'), name='total_loss')
        
# training
def train(total_loss, global_step):
    # learning parameters
    # Decay the learning rate exponentially based on the number of steps.
    lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,
                                  global_step,
                                  decay_steps,
                                  LEARNING_RATE_DECAY_FACTOR,
                                  staircase=True)
    tf.summary.scalar('learning_rate', lr)
    
    # Generate moving averages of all losses and associated summaries.
    # Compute gradients.
    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')
    losses = tf.get_collection('losses')
    loss_averages_op = loss_averages.apply(losses + [total_loss])
      # Compute gradients.
    with tf.control_dependencies([loss_averages_op]):
#        opt = tf.train.GradientDescentOptimizer(learning_rate=lr)
#        grads = opt.compute_gradients(total_loss)    
#        grads = -WEIGHT_DACAY*lr*weights+ lr * grads
        opt = tf.train.MomentumOptimizer(learning_rate=lr, momentum=MOMENTUM,name='Momentum')
        grads = opt.compute_gradients(total_loss)

    # Apply gradients.
    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)
    
    # Track the moving averages of all trainable variables.
    
    variable_averages = tf.train.ExponentialMovingAverage(
            MOVING_AVERAGE_DECAY, global_step)
    variables_averages_op = variable_averages.apply(tf.trainable_variables())

    # Get optimal training parameters
    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):
        train_op = tf.no_op(name='train')
    

    return train_op

def run_alexnet_train_proc(x, y, val_x, val_y, sess):
    batchsize = 10
    n_epoch = 10
    num_batches = 100
    global_step = tf.Variable(0, name='global_step', trainable=False)
    for i in xrange(num_batches):
        total_loss = 0.0
        softmax_linear, parameters = model(images=x(i), train=True)
        total_loss = loss(output=softmax_linear, parameters=parameters,
                          label=y(i), train=True)
        train(total_loss=total_loss, global_step=global_step)
        sess.run([parameters, total_loss])
    return parameters

def main():
    
    # trainning data
    train_x, train_y, val_x, val_y = initialize(dataset='MNIST')
    
    init = tf.global_variables_initializer()
    
    #get minibatch
    session = tf.Session()
    session.run(init)
    run_alexnet_train_proc(x=train_x, y=train_y, val_x=val_x, val_y=val_y, session)
    # testing erros
    
    logits, params = model(x)
    loss = loss(logits, label_x)
    
    

    
    return

if __name__ = '__main__' :
    
# 